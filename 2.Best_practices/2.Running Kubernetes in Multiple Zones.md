

# 📘 Running Kubernetes in Multiple Zones - Summary

## 🔍 **Background**

- Kubernetes supports running a **single cluster across multiple failure zones** (or availability zones) within a **region**.
- A **region** (as defined by cloud providers like AWS, GCP, Azure) consists of **multiple zones**, each offering similar services and APIs.
- **Purpose**: Increase **availability** and **resilience**. If one zone fails, other zones can continue functioning.

---

## 🧠 **Why Multi-Zone Setup Matters?**

- **Zone-level isolation** ensures that the failure in one zone doesn't impact the whole cluster.
- Example: If zone `us-central1-a` goes down, nodes in zones `us-central1-b` and `us-central1-c` continue running.

---

## 🧩 **Control Plane Behavior**

- Control plane components:
  - `API Server`
  - `Scheduler`
  - `etcd`
  - `Controller Manager`
  - (Optionally) `Cloud Controller Manager`

- To improve availability:
  - Deploy **replicas of each control plane component across at least 3 zones**.
  - This ensures redundancy if one zone fails.

> ⚠️ **Important:** Kubernetes itself does not offer **cross-zone failover** for API server endpoints.
>
> 🛠️ Use external tools like:
> - DNS Round Robin
> - SRV Records
> - Load Balancers with health checks

---

## 🧬 **Node Behavior and Pod Scheduling**

- Kubernetes **spreads pods across nodes automatically**.
- Nodes get **zone labels** (e.g., `topology.kubernetes.io/zone=us-central1-a`) via the kubelet.
- You can use:
  - **Node labels**
  - **Topology spread constraints**
  - **Affinity/Anti-affinity rules**

### 🧪 Example:
```yaml
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: topology.kubernetes.io/zone
  whenUnsatisfiable: ScheduleAnyway
  labelSelector:
    matchLabels:
      app: my-app
```
> Ensures that pods of `my-app` are evenly spread across zones.

---

## 🌍 **Distributing Nodes Across Zones**

- Kubernetes **doesn’t provision nodes**—you must:
  - Manually provision nodes in each zone
  - Use tools like **Cluster API** to automate this

> 🔁 Cluster API can:
> - Create node pools in multiple zones
> - Heal the cluster if a whole zone fails

---

## 📦 **Manual Zone Assignment for Pods**

- Use **`nodeSelector`** or **`affinity`** to pin pods to a specific zone or node.
  
### 📄 Example:
```yaml
spec:
  nodeSelector:
    topology.kubernetes.io/zone: us-central1-a
```

---

## 💾 **Storage Access in Multi-Zone Setup**

- Kubernetes adds **zone labels to PersistentVolumes**.
- Scheduler uses **NoVolumeZoneConflict** logic:
  - A pod using a volume is only scheduled to the **same zone** where that volume exists.

> ⚠️ Cloud-specific behavior:
> - Some storage provisioners are **zone-aware**
> - Check your **cloud provider’s documentation** for accurate config

### 📘 Example:
To ensure volumes are created only in specific zones:
```yaml
storageClass:
  allowedTopologies:
  - matchLabelExpressions:
    - key: topology.kubernetes.io/zone
      values:
        - us-central1-a
        - us-central1-b
```

---

## 🌐 **Networking in Multi-Zone Clusters**

- Kubernetes **does not provide zone-aware networking by default**.
- You need to rely on:
  - **CNI plugins**
  - **Cloud provider's LoadBalancers** (e.g., GCP/AWS)

> ⚠️ A LoadBalancer might only route traffic to pods **in the same zone** where the LB is deployed.

---

## 🧯 **Fault Recovery in Multi-Zone Failures**

> 🤔 What if **all zones in a region fail**?

- You must plan for:
  - **Cluster repair jobs** that do not rely on a healthy node
  - Use **tolerations** in your repair Pods

### 📄 Example:
```yaml
tolerations:
- key: "node.kubernetes.io/not-ready"
  operator: "Exists"
  effect: "NoExecute"
```

---

## 🧭 **What’s Next?**

- Learn how **Pod scheduling and eviction** works.
- Explore:
  - [`VerticalPodAutoscaler`](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
  - [`Cluster API`](https://cluster-api.sigs.k8s.io/)
  - [`Topology Spread Constraints`](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)
  - [`StorageClasses with topology`](https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies)

---

## ✅ **Key Takeaways for Exam**

- Deploy control plane components across 3 zones for HA.
- Use node labels and spread constraints to distribute pods.
- Use storage classes aware of failure zones.
- Use Cluster API for node automation and healing.
- Plan recovery strategies for full region outage.
- Use custom tolerations for critical repair jobs.

---

