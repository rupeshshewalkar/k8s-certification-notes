
# ‚úÖ Summary: Set Up High Availability (HA) etcd Cluster with `kubeadm`

## üß† Why This Matters
- `etcd` is the key-value store used by Kubernetes for all cluster data.
- In **HA mode**, running multiple etcd instances ensures **data durability and availability** even if one node fails.
- By default, `kubeadm` runs etcd *locally* on each control plane node, but we can also set up etcd as an *external cluster*.

This guide explains how to manually provision a **3-node external etcd cluster** using `kubeadm`.

---

## üõ† Prerequisites

1. **3 Hosts** that can communicate on **TCP ports 2379 and 2380** (default etcd ports).
2. **Each host must have:**
   - `systemd` and a `bash`-compatible shell
   - A container runtime (like containerd)
   - `kubelet` and `kubeadm` installed
   - Internet access to pull images from `registry.k8s.io`
3. **Tools to copy files** (e.g., `ssh`, `scp`) between hosts.
4. `kubeadm` will generate all necessary certificates ‚Äî no other crypto tools needed.

---

## üèóÔ∏è Step-by-Step Setup

### üîß Step 1: Configure Kubelet to Manage etcd

On **each etcd host**, set up kubelet to manage etcd as a **static pod**.

```bash
cat <<EOF > /etc/systemd/system/kubelet.service.d/kubelet.conf
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: false
authorization:
  mode: AlwaysAllow
cgroupDriver: systemd
address: 127.0.0.1
containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock
staticPodPath: /etc/kubernetes/manifests
EOF
```

Create a new systemd unit to override default kubeadm kubelet service:

```bash
cat <<EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
ExecStart=/usr/bin/kubelet --config=/etc/systemd/system/kubelet.service.d/kubelet.conf
Restart=always
EOF
```

Reload systemd and restart kubelet:

```bash
systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet
```

---

### üìÑ Step 2: Create `kubeadm` Config Files for Each Host

Set hostnames and IPs for all 3 nodes:

```bash
export HOST0=10.0.0.6
export HOST1=10.0.0.7
export HOST2=10.0.0.8
export NAME0="infra0"
export NAME1="infra1"
export NAME2="infra2"
```

Create temp folders to hold generated configs:

```bash
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/
```

Generate a `kubeadmcfg.yaml` file for each host using a loop:

```bash
HOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=(${NAME0} ${NAME1} ${NAME2})

for i in "${!HOSTS[@]}"; do
HOST=${HOSTS[$i]}
NAME=${NAMES[$i]}
cat <<EOF > /tmp/${HOST}/kubeadmcfg.yaml
# kubeadm Init and Cluster Configuration
apiVersion: "kubeadm.k8s.io/v1beta4"
kind: InitConfiguration
nodeRegistration:
  name: ${NAME}
localAPIEndpoint:
  advertiseAddress: ${HOST}
---
apiVersion: "kubeadm.k8s.io/v1beta4"
kind: ClusterConfiguration
etcd:
  local:
    serverCertSANs:
    - "${HOST}"
    peerCertSANs:
    - "${HOST}"
    extraArgs:
    - name: initial-cluster
      value: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380
    - name: initial-cluster-state
      value: new
    - name: name
      value: ${NAME}
    - name: listen-peer-urls
      value: https://${HOST}:2380
    - name: listen-client-urls
      value: https://${HOST}:2379
    - name: advertise-client-urls
      value: https://${HOST}:2379
    - name: initial-advertise-peer-urls
      value: https://${HOST}:2380
EOF
done
```

---

### üîê Step 3: Generate Certificate Authority (CA)

> Skip this step if you already have a CA.

Run on `HOST0`:

```bash
kubeadm init phase certs etcd-ca
```

Creates:
- `/etc/kubernetes/pki/etcd/ca.crt`
- `/etc/kubernetes/pki/etcd/ca.key`

---

### üìú Step 4: Generate Certificates for Each Host

On each host:

```bash
kubeadm init phase certs etcd-server --config=/tmp/${HOST}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST}/kubeadmcfg.yaml
```

Copy the `/etc/kubernetes/pki` folder to respective host's `/tmp/${HOST}` directory.

Delete all non-reusable certs (except `ca.crt` and `ca.key` on HOST0).

Example:
```bash
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
```

---

### üìÅ Step 5: Copy Files to Other Nodes

From `HOST0`, transfer files to HOST1 and HOST2:

```bash
scp -r /tmp/${HOST1}/* ubuntu@${HOST1}:
scp -r /tmp/${HOST2}/* ubuntu@${HOST2}:
```

On remote host:

```bash
sudo -Es
chown -R root:root pki
mv pki /etc/kubernetes/
```

---

### ‚úÖ Step 6: Validate Required Files Exist

Each host must have:

- `/etc/kubernetes/pki/etcd/` with `ca.crt`, `server.crt`, `peer.crt`, `healthcheck-client.crt`, and related `.key` files.
- `apiserver-etcd-client.crt` and `.key`
- `kubeadmcfg.yaml`

---

### üì¶ Step 7: Create etcd Static Pod Manifests

On **each host**, generate etcd static pod manifests using:

```bash
kubeadm init phase etcd local --config=/tmp/${HOST}/kubeadmcfg.yaml
```

> These manifests will be placed in `/etc/kubernetes/manifests`, and kubelet will automatically launch etcd containers from them.

---

## üß™ (Optional) Check etcd Cluster Health

If `etcdctl` is not available on your system, run it in a temporary etcd container.

Example:

```bash
ETCDCTL_API=3 etcdctl \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/peer.crt \
  --key=/etc/kubernetes/pki/etcd/peer.key \
  --endpoints=https://127.0.0.1:2379 endpoint health
```

---

## üìå Notes

- You **must run these steps on all 3 nodes**.
- This is a **manual setup** but gives you **fine-grained control** over the HA etcd topology.
- If any host fails, the remaining two can still maintain quorum and keep the cluster running.

---

## üí° Example

Imagine you have three VMs:
- VM1 (10.0.0.6 / infra0)
- VM2 (10.0.0.7 / infra1)
- VM3 (10.0.0.8 / infra2)

This process will:
- Configure each as an etcd node
- Set up secure certs for intra-node communication
- Run etcd via kubelet-managed static pods
- Result in a resilient etcd cluster usable by your Kubernetes control plane.
